{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2290d-85a3-4e97-8be4-5665314d0fcd",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "collection=\"recommendRoutines\"\n",
    "dt='19700101'\n",
    "env=\"stg\"\n",
    "buffer_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dab09-90f8-49e9-811c-9b2bbb8dbd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import re\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from skt.ye import get_secrets\n",
    "from skt.gcp import get_bigquery_client\n",
    "# from skt.gcp import get_temp_table\n",
    "\n",
    "mongodb_access_info = get_secrets(\"adot/routine_db/mongodb_access\")\n",
    "db_user = mongodb_access_info[f\"{env}_user\"]\n",
    "db_password = mongodb_access_info[f\"{env}_password\"]\n",
    "\n",
    "# STG\n",
    "mongodb_URI = f\"mongodb://{db_user}:{db_password}@172.27.8.210:27017/\"\n",
    "\n",
    "# Prod\n",
    "mongodb_prod_URI = f\"mongodb://{db_user}:{db_password}@172.27.99.85:27017,172.27.99.86:27017,172.27.99.87:27017,172.27.99.88:27017,172.27.99.89:27017/\"\n",
    "\n",
    "bq_table = re.sub(r'(?<!^)(?=[A-Z])', '_', collection).lower()\n",
    "bq = get_bigquery_client()\n",
    "\n",
    "database=\"\"\n",
    "bigquery_destination=\"\"\n",
    "if env==\"prd\":\n",
    "    #client\n",
    "    client = MongoClient(mongodb_prod_URI)\n",
    "    #database\n",
    "    database=client[\"prd_nugu\"]\n",
    "    bigquery_destination=f\"{bq.project}.adot_routine_prd.{bq_table}\"\n",
    "else:\n",
    "    #client\n",
    "    client = MongoClient(mongodb_URI)\n",
    "    #database\n",
    "    database=client[\"stg_nugu\"]\n",
    "    bigquery_destination=f\"{bq.project}.adot_routine_stg.{bq_table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5402b-ed9d-4a13-8bb4-450478455b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.bigquery import RangePartitioning, PartitionRange, TimePartitioning\n",
    "from google.cloud.bigquery.job import QueryJobConfig, LoadJobConfig\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.cloud import bigquery\n",
    "from skt.gcp import *\n",
    "\n",
    "#little modified from skt pandas_to_bq library\n",
    "#added component list\n",
    "#1.\n",
    "#parquet_options = bigquery.format_options.ParquetOptions()\n",
    "#parquet_options.enable_list_inference = True\n",
    "#2.\n",
    "#pyarrow.Table.from_pandas(pd_df[pd_df[partition] == part_val],preserve_index=False)\n",
    "def pandas_to_bq(pd_df, destination, partition=None, clustering_fields=None, overwrite=True):\n",
    "    range_partitioning = None\n",
    "    time_partitioning = None\n",
    "    bq = get_bigquery_client()\n",
    "    if bq_table_exists(destination):\n",
    "        target_table = bq.get_table(destination)\n",
    "        range_partitioning = target_table.range_partitioning\n",
    "        time_partitioning = target_table.time_partitioning\n",
    "    else:\n",
    "        if partition:\n",
    "            from pandas.api.types import is_integer_dtype\n",
    "            import datetime\n",
    "\n",
    "            if is_integer_dtype(pd_df[partition][0]):\n",
    "                range_partitioning = RangePartitioning(\n",
    "                    PartitionRange(start=200001, end=209912, interval=1), field=partition\n",
    "                )\n",
    "            elif isinstance(pd_df[partition][0], datetime.date):\n",
    "                time_partitioning = TimePartitioning(field=partition)\n",
    "            else:\n",
    "                raise Exception(\"Partition type must be either date or range.\")\n",
    "\n",
    "    parquet_options = bigquery.format_options.ParquetOptions()\n",
    "    parquet_options.enable_list_inference = True\n",
    "    \n",
    "    if time_partitioning or range_partitioning:\n",
    "        if time_partitioning:\n",
    "            input_partitions = [(p.strftime(\"%Y%m%d\"), p) for p in set(pd_df[partition])]\n",
    "        elif range_partitioning:\n",
    "            input_partitions = [(p, p) for p in set(pd_df[partition])]\n",
    "        for part, part_val in input_partitions:\n",
    "            writer = pyarrow.BufferOutputStream()\n",
    "            pyarrow.parquet.write_table(\n",
    "                pyarrow.Table.from_pandas(pd_df[pd_df[partition] == part_val],preserve_index=False),\n",
    "                writer\n",
    "            )\n",
    "            reader = pyarrow.BufferReader(writer.getvalue())\n",
    "            \n",
    "            bq.load_table_from_file(\n",
    "                reader,\n",
    "                destination=f\"{destination}${part}\",\n",
    "                job_config=LoadJobConfig(\n",
    "                    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "                    write_disposition=\"WRITE_TRUNCATE\" if overwrite else \"WRITE_APPEND\",\n",
    "                    time_partitioning=time_partitioning,\n",
    "                    range_partitioning=range_partitioning,\n",
    "                    clustering_fields=clustering_fields,\n",
    "                    source_format = bigquery.SourceFormat.PARQUET,\n",
    "                    parquet_options = parquet_options,\n",
    "                    schema_update_options=[\n",
    "                        \"ALLOW_FIELD_ADDITION\",\n",
    "                        \"ALLOW_FIELD_RELAXATION\",\n",
    "                    ],\n",
    "                ),\n",
    "            ).result()\n",
    "    else:\n",
    "        bq.load_table_from_dataframe(\n",
    "            dataframe=pd_df,\n",
    "            destination=destination,\n",
    "            job_config=LoadJobConfig(\n",
    "                create_disposition=\"CREATE_IF_NEEDED\",\n",
    "                write_disposition=\"WRITE_TRUNCATE\" if overwrite else \"WRITE_APPEND\",\n",
    "            ),\n",
    "        ).result()\n",
    "    bq.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb42c6-e6ce-488f-a753-7a8b36f13671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_document(x):\n",
    "    x[\"_id\"] = str(x[\"_id\"])\n",
    "    # if \"modifiedAt\" in x and isinstance(x[\"modifiedAt\"], str):\n",
    "    #     x[\"modifiedAt\"] = datetime.datetime.strptime(x[\"modifiedAt\"], \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    # if \"createAt\" in x and isinstance(x[\"createAt\"], str):\n",
    "    #     x[\"createAt\"] = datetime.datetime.strptime(x[\"createAt\"], \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    if \"modifiedAt\" in x and isinstance(x[\"modifiedAt\"], dict):\n",
    "        x[\"modifiedAt\"] = x[\"modifiedAt\"][\"$date\"]\n",
    "    if \"createAt\" in x and isinstance(x[\"createAt\"], dict):\n",
    "        x[\"createAt\"] = x[\"createAt\"][\"$date\"]\n",
    "    if collection == \"userRoutine\":\n",
    "        x[\"actions\"] = json.dumps(x[\"actions\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d19c6-4af0-4726-9a86-c26a58ca3e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database[collection].count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8fcd1-e9be-41ed-b67c-54acf22d0954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_df_to_bq_temp_table(df, dest_table):\n",
    "    if \"expiredAt\" in df.columns:\n",
    "        df[\"expiredAt\"]=df[\"expiredAt\"].astype(\"datetime64[ns]\")\n",
    "    if \"createAt\" in df.columns:\n",
    "        df[\"createAt\"]=df[\"createAt\"].astype(\"datetime64[ns]\")\n",
    "    if \"modifiedAt\" in df.columns:\n",
    "        df[\"modifiedAt\"]=df[\"modifiedAt\"].astype(\"datetime64[ns]\")\n",
    "    if \"snoozeAt\" in df.columns:\n",
    "        df[\"snoozeAt\"]=df[\"snoozeAt\"].astype(\"datetime64[ns]\")\n",
    "    if collection==\"recommendRoutine\":\n",
    "        df[\"priority\"]=df[\"priority\"].astype(\"bool\")\n",
    "        df[\"enable\"]=df[\"enable\"].astype(\"bool\")\n",
    "    df[\"dt\"]=datetime.date(int(dt[0:4]),int(dt[4:6]),int(dt[6:]))\n",
    "    pandas_to_bq(df, dest_table, partition=\"dt\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632385c5-76b4-4aed-ba21-141f2dab374e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skt.gcp import get_temp_table\n",
    "\n",
    "inputs = database[collection].find()\n",
    "\n",
    "temp_table = get_temp_table()\n",
    "buffer = []\n",
    "\n",
    "for x in inputs:\n",
    "    buffer.append(x)\n",
    "    if len(buffer) == buffer_size:\n",
    "        buffer = list(map(process_document, buffer))\n",
    "        df=pd.DataFrame(buffer)\n",
    "        append_df_to_bq_temp_table(df, temp_table)\n",
    "        buffer = []\n",
    "if buffer:\n",
    "    buffer = list(map(process_document, buffer))\n",
    "    df=pd.DataFrame(buffer)\n",
    "    append_df_to_bq_temp_table(df, temp_table)\n",
    "    buffer = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db0448-02aa-4f42-aa4b-dff7ddd368bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49670b-3bd4-4007-9bfe-1d79d7ee2d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert(source, target, dt):\n",
    "    from google.cloud.bigquery.table import TableReference\n",
    "    from google.cloud.bigquery.dataset import DatasetReference\n",
    "    from google.cloud.bigquery.job import QueryJobConfig\n",
    "    from skt.gcp import _print_query_job_results\n",
    "    \n",
    "    bq = get_bigquery_client()\n",
    "    table = bq.get_table(target)\n",
    "    project_id, dataset_id, table_id = target.split(\".\")\n",
    "    ref = TableReference(DatasetReference(project_id, dataset_id), f\"{table_id}${dt}\")\n",
    "    qjc = QueryJobConfig(\n",
    "        destination=ref,\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        create_disposition=\"CREATE_IF_NEEDED\",\n",
    "        time_partitioning=table.time_partitioning,\n",
    "        range_partitioning=table.range_partitioning,\n",
    "        clustering_fields=table.clustering_fields,\n",
    "        schema_update_options=[\n",
    "            \"ALLOW_FIELD_ADDITION\",\n",
    "            \"ALLOW_FIELD_RELAXATION\",\n",
    "        ]\n",
    "    )\n",
    "    query = f\"select * from {source}\"\n",
    "    job = bq.query(query, job_config=qjc)\n",
    "    job.result()\n",
    "    _print_query_job_results(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c8045-ce41-47e5-95c7-c8a3119fc964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insert(temp_table, bigquery_destination, dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
